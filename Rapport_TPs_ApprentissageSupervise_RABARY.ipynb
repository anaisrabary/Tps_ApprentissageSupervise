{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_metadata": {
     "author": "Anaïs RABARY",
     "date": "2018-12-10",
     "output": {
      "pdf_document": {
       "toc": true
      }
     },
     "subtitle": "Etude comparative des méthodes KNN, ANN et MLP",
     "title": "Rapport TPs Apprentissage Supervisé"
    }
   },
   "source": [
    "# Rapport de TPs - Apprentissage Supervisé (KNN-ANN-SVM)\n",
    "\n",
    "## Auteur : Anaïs RABARY (5SDBD - Gr A)\n",
    "### Date : 7 Décembre 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "NOTES : Le beau rapport avec un plan et des images dimensionnées ainsi que les codes des TP 1, 2 et 3 sont disponibles sur ce git : https://github.com/anaisrabary/Tps_ApprentissageSupervise .\n",
    "\n",
    "## Introduction\n",
    "<a id=\"intro\"></a>\n",
    "Dans le cadre des Travaux Pratiques sur l'Apprentissage Supervisé réalisés en 5ème Année SDBD (Système Dsitribués Big Data), sous la supervision de M-V. Le Lann et M. Siala, nous avons étudiés différentes méthodes d'apprentissage supervisé : KNN, K-Neighrest Neighbors ou les algo des K plus proches voisins, ANN, Artificial Neural Network ou Réseau Artificiel de Neurones et SVM, Support Vector Machine ou machine à support de vecteur. L'étude est réalisée à partir de l'outil [Scikit-Learn sur python](https://scikit-learn.org/).\n",
    "\n",
    "L'objectif de ce rapport est de présenter ces 3 méthodes puis de les comparer. Ce rapport se veut pédagogique pour former un support personnel qui pourra être réutilisé pour des études futures. On présentera notamment les paramètres entrant dans l'étude de comparaison. Les codes exécutés pour réaliser l'étude seront rendus dans leur totalité dans les autres notebook. Je n'ai pas jugé nécessaire de surcharger ce rapport et d'entraver sa bonne lisibilité.\n",
    "\n",
    "Ces 3 méthodes ont pour but de classifier les données. Afin de pouvoir les évaluer, nous travaillerons sur un même jeu de données, [MNIST](http://yann.lecun.com/exdb/mnist/), base de données ouvertes de chiffres écrits à la main. Dans ce cas, le but sera d'arriver à déterminer les 10 classes de chiffres. \n",
    "Les éléments de comparaison dépendront des méthodes, mais nous analyserons à chaque fois leurs performances et leur temps d'éxécution. On s'attardera aussi sur les choix des paramètres permettant d'obtenir les meilleurs résultats pour la classification sur le jeu de données mnist.\n",
    "\n",
    "\n",
    "## PLAN\n",
    "\n",
    "\n",
    "[Introduction](#introduction)  \n",
    "1. [Présentations générales](#markdown-header-présentations-générales)  \n",
    "    1.1 [Le Dataset](#dataset)  \n",
    "    1.2 [KNN](#KNN)  \n",
    "    1.3 [ANN](#ANN)   \n",
    "    1.4 [SVM](#SVM)  \n",
    "2. [Etudes séparées](#etud)  \n",
    "   2.1 [Méthodes de vérification](#verif)  \n",
    "   2.2 [Indicateurs de performance](#perf)  \n",
    "   2.3 [Choix des paramètres de l'algorithme](#choix)  \n",
    "      2.3.1 [Mnist et KNN](#paramKNN)  \n",
    "      2.3.2 [Mnist et les neuronnes](#paramANN)  \n",
    "      2.3.3 [Mnist et SVM](#paramSVM)  \n",
    "3. [Analyse comparative](#comp)  \n",
    "[Conclusion](#conclusion)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Présentations générales\n",
    "<a id=\"presgen\"></a>\n",
    "\n",
    "### 1.1 Le Dataset\n",
    "<a id=\"dataset\"></a>\n",
    "\n",
    "Le dataset utilisé pour la comparaison des algorithmes, les chiffres de 0 à 9 écrits à la main, contient 70 000 images labélisées. Ces images font 28x28 pixels, et donc comportent 784 pixels. \n",
    "L'objectif des algorithmes développés ici est de pouvoir classifier ces images et ainsi pouvoir prédire le chiffre d'une image non labélisée. \n",
    "Ci-après voici un exemple d'affichage des données mnist.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-5\">\n",
    "        <img src=\"images_rapport/MnistExamples.png\" style=\" width: 300px;\" />\n",
    "    </div>\n",
    "    <div class=\"col-md-5\">\n",
    "        <img src=\"images_rapport/mnist_example.png\" style=\" width: 200px;\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Ce jeu de données a un but pédagogique. Les 70000 images sont donc toutes labélisées. On veillera donc à séparer notre jeu en 2 parties : une partie pour l'apprentissage et une partie pour le test. Grâce aux labels des données de tests, on pourra mesurer le score de notre modèle. Le jeu comprends plusieurs attributs dont notamment data et target qui vont nous être utiles pour apprendre, tester et vérifier nos models:\n",
    "- data, un tableau de n instances x m attributs\n",
    "- target, le label associé à chaque instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 KNN\n",
    "<a id=\"KNN\"></a>\n",
    "\n",
    "On rappel ici les principes de KNN :\n",
    "L'algorithme des k-plus proches voisins, ou K Neighrest Neighbors, est un algorithme simple et intuitif. Il est utilisé pour des problèmes de classification.\n",
    "Pour chaque objet $o$ que l'on souhaite classifier, on cherche ses $K$ plus proches voisins connus (labélisés). Puis on associe à $o$ le label qui est majoritaire parmis ses voisins. \n",
    "\n",
    "On effectue les tests en utilisant le model de scikit-learn, sklearn.neighbors, qui est paramétrable.\n",
    "\n",
    "Pour trouver les K plus proche voisins, l'algorithme KNN repose sur une mesure de distance. Cependant, il existe plusieurs types de distances différentes :\n",
    "- Euclidienne : $\\sqrt{\\sum (x-y)^2}$\n",
    "- Manhattan : $\\sum|x-y|$\n",
    "- Chebyshev :$max(|x-y|)$\n",
    "- Minkowski :$(\\sum|x-y|^p)^{1/p}$\n",
    "- Hamming : $\\sum|x-y|$ avec $x=y \\implies D=0$ et $x\\neq y \\implies D=1$\n",
    "\n",
    "Par défaut, c'est la distance euclidienne qui est utilisée. Une étude comparative de ces distances est réalisée dans la partie 2.3.1.\n",
    "\n",
    "Voici ci-après le squelette de code qui servira à l'étude des performances de KNN sur Mnist. On rappelle que la totalité du code ayant servie à l'étude de KNN est donnée dans le notebook dédié.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports liés à KNN et Mnist\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import neighbors\n",
    "#Imports pour la mesure de performance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "# Charger les données\n",
    "mnist=datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "#ECHANTILLONS DE 10000\n",
    "indices = np.random.randint(70000, size=10000)\n",
    "data = mnist.data[indices]\n",
    "target = mnist.target[indices]\n",
    "#pour séparer le dataset en 2 échantillons de training et de test\n",
    "#Data fait donc 8000 et test 2000\n",
    "xtrain, xtest, ytrain, ytest =train_test_split(data, target, train_size=0.8)\n",
    "\n",
    "# Classifier KNN, apprentisage, prédiction et score\n",
    "clf=neighbors.KNeighborsClassifier(10)\n",
    "clf.fit(xtrain,ytrain)\n",
    "predict = clf.predict(xtest)\n",
    "score = clf.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 ANN\n",
    "<a id=\"ANN\"></a>\n",
    "On présente ici un réseau de neurones multicouches, perceptron multi-couche (MLP, Multi-layered percepron ou Artificial Neural Network). On peut distinguer 3 parties :\n",
    "- les entrées : permettent de prendre un vecteur à plusieurs dimensions (une dimension par neurone),\n",
    "- les sorties : présentent le résultat,\n",
    "- les couches cachées (hidden layers) : permettes de faire une séparation entre les données, par combinaison linéaire.\n",
    "\n",
    "Sur les liens entre chaques neurones se trouvent des **poids** $w_0,w_1,w_2$ (weight). C'est ces poids qui sont adaptés pendant l'apprentissage. Dans un neurone, on cacule la somme pondérée de tous les signaux entrants, $\\sum(w_i\\dot x_i)$. Puis on y ajoute un **biais** $b$, qui permet de décaller la fonction d'activation. En sortant du neurone, le signal calculé passe par une **fonction d'activation** qui laisse passer un signal suivant un seuil. On a enfin le signal de sortie du neuronne.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 10px\">\n",
    "    <div class=\"col-md-5\">\n",
    "        <img src=\"images_rapport/MLP/MLPSchema.jpg\" style=\" width: 300px;\" />\n",
    "    </div>\n",
    "    <div class=\"col-md-5\">\n",
    "        <img src=\"images_rapport/MLP/neurone_detail.png\" style=\"width: 250px;\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Lors de la création d'un tel réseau, les poids et biais sont initialisés aléatoirement. Puis c'est par une méthode de backpropagation qu'ils sont recalculés jusqu'à la fin de l'apprentissage.\n",
    "\n",
    "Le modèle proposé par scikit learn est paramétrable.  \n",
    "Pour les neurones dans les couhces cachés, il y a plusieurs fonction d'activation possibles :\n",
    "- identity, $f(x)=x$ donc pas de fonction d'activation,\n",
    "- logistic, $f(x)=1/(1+\\exp (-x))$, la fonction d'activation du sigmoid,\n",
    "- tanh, $f(x)=tanh(x)$, la fonction hyperboliques de tan,\n",
    "- relu, $f(x)=max(0,x)$, la fonction de rectification linéaire unitaire.\n",
    "C'est relu qui est utilisé par défaut.  \n",
    "\n",
    "On peut aussi modifier le solver utilisé pour calculer les poids :\n",
    "- lbfgs, un optimiseur de la famille des méthodes quasi Newtonienne,\n",
    "- sgd, la méthode de déscente de gradient stochastic,\n",
    "- adam, une autre méthode de descente de gradient, optimisée par Kingma, Diederik et Jimmmy Ba.\n",
    "Par défaut, c'est adam qui est utilisé.\n",
    "\n",
    "Il est aussi possible de paramétrer alpha $\\alpha$, qui est le paramètre de régularisation dans la pénalité L2. Il permet de prévenir l'overfitting, en contraignant la taille des poids. \n",
    "\n",
    "Choisir une architecture de réseau de neurone demande une certaine expertise. Il faut essayer différentes architectures et choisir le réseau donnant le meilleur résultat de généralisation sur les tests.\n",
    "\n",
    "\n",
    "\n",
    "Voici ci-après le squelette de code qui servira à l'étude des performances de ANN sur Mnist. On rappelle que la totalité du code ayant servie à l'étude de ANN est donnée dans le notebook dédié.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessaires\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "#charger le jeu de données MNIST\n",
    "mnist=datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "#randomise Data et target\n",
    "indices = np.random.randint(70000, size=70000)\n",
    "data = mnist.data[indices]\n",
    "target = mnist.target[indices]\n",
    "# on veut un training set de 49000\n",
    "xtrain, xtest, ytrain, ytest =train_test_split(data, target, train_size=49000)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50))\n",
    "clf.fit(xtrain, ytrain)\n",
    "predict = clf.predict(xtest)\n",
    "score = clf.score(xtest,ytest)\n",
    "\n",
    "recall = metrics.recall_score(ytest, predict, average ='macro')\n",
    "precision = metrics.precision_score(ytest, predict,  average='macro')\n",
    "loss01 = metrics.zero_one_loss(ytest, predict)\n",
    "\n",
    "\n",
    "print(\"Ce modèle de MLP, d'1 couche de 50, à un score de \", score*100, \"%.\")\n",
    "print(\"4eme image : prédiction \",predict[3], \"reel : \", ytest[3])\n",
    "print (\"ce modèle de MLP à une précision de\", precision*100, \"%.\")\n",
    "print (\"ce modèle de MLP à un recall de\",recall*100, \"%.\")\n",
    "print (\"ce modèle de MLP à un zero-one_loss de\",recall*100, \"%.\")\n",
    "print(\"    temps apprentissage : \", timetrain, \"sec , temps prediction = \", timePred, \"sec.\" )\n",
    "\n",
    "\n",
    "#Ce modèle de MLP, d'1 couche de 50, à un score de  98.5047619047619 %.\n",
    "#4eme image : prédiction  5.0 reel :  5.0\n",
    "#ce modèle de MLP à une précision de 98.50503688809177 %.\n",
    "#ce modèle de MLP à un recall de 98.49104431028593 %.\n",
    "#ce modèle de MLP à un zero-one_loss de 98.49104431028593 %.\n",
    "#    temps apprentissage :  322.9891335964203 sec , temps prediction =  0.6702065467834473 sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 SVM\n",
    "<a id=\"SVM\"></a>\n",
    "\n",
    "La méthode d'apprentissage par Machine à Vecteurs de Support ou SVM est la dernière méthode que nous allons voir dans ce rapport. Le but de SVM est de maximiser la marge de séparation entre les classes. Cela permet en fait de séparer linéairement nos données en augmentant la dimension. La méthode permet de trouver un hyperplan qui sépare nos données, tout en maximisant les marges.\n",
    "\n",
    "<div class=\"row\">\n",
    "     <img src=\"images_rapport/SVM/SVM_2.png\" style=\" width: 300px;\" />\n",
    "</div>\n",
    "\n",
    "Nous allons utiliser SVC de Scikit-learn.\n",
    "\n",
    "Il est possible de faire varier plusieurs paramètres. \n",
    "**Le kernel** :\n",
    "- linéaire\n",
    "- poly\n",
    "- rbf\n",
    "- sigmoid\n",
    "Par défaut c'es RBF qui est utilisé\n",
    " \n",
    "Il est aussi possible de faire varier **le coût** et **gamma**.\n",
    "\n",
    "Voici ci-après le squelette de code qui servira à l'étude des performances de SVM sur Mnist. On rappelle que la totalité du code ayant servie à l'étude de SVM est donnée dans le notebook dédié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessaires\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#charger le jeu de données MNIST\n",
    "mnist=datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "#randomise Data et target\n",
    "indices = np.random.randint(10000, size=10000)\n",
    "data = mnist.data[indices]\n",
    "target = mnist.target[indices]\n",
    "#pour séparer le dataset en 2 échantillons de trainint et de test\n",
    "# on veut un training set de 49000 (70% training set - 30 % test set)\n",
    "xtrain, xtest, ytrain, ytest =train_test_split(data, target, train_size=int(len(data)*0.7))\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "startTrain =time.time()\n",
    "clf.fit(xtrain, ytrain)\n",
    "endTrain = time.time()\n",
    "# PREDIC\n",
    "startpred= time.time()\n",
    "predict = clf.predict(xtest)\n",
    "endpred = time.time()\n",
    "# METRICS\n",
    "score = clf.score(xtest,ytest)\n",
    "recall = metrics.recall_score(ytest, predict, average ='macro')\n",
    "precision = metrics.precision_score(ytest, predict,  average='macro')\n",
    "loss01 = metrics.zero_one_loss(ytest, predict)\n",
    "timetrain = endTrain - startTrain\n",
    "timePred = endpred - startpred\n",
    "\n",
    "print(\"Ce modèle SVC avec un kernel\", ker, \"a un score de \", score*100, \"%.\")\n",
    "print(\"4eme image : prédiction \",predict[3], \"reel : \", ytest[3])\n",
    "print (\"précision :\", precision*100)\n",
    "print (\"recall  :\",recall*100)\n",
    "print (\"zero-one_loss :\",recall*100)\n",
    "print( \"training time :\", timetrain)\n",
    "print( \"prediction time :\", timePred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Etudes séparées\n",
    "<a id=\"etud\"></a>\n",
    "\n",
    "### 2.1 Méthode de vérification\n",
    "<a id=\"verif\"></a>\n",
    "\n",
    "Il existe 3 grandes méthodes de vérifications : \n",
    "-\tTest set validation (holdout method)\n",
    "-\tK-fold cross-validation\n",
    "-\tLeave-one-out cross-validation\n",
    "\n",
    "\n",
    "La 1ère, Test Set validation est la plus simple. Elle consiste à séparer le jeu de données en 2. Une partie est réservée pour l'apprentissage, l'autre pour le test et donc la validation du modèle. Cela peut être par exemple, réserver 70% des données pour l'apprentissage et les 30% restants pour les tests.\n",
    "\n",
    "Cependant, si l'on changeait nos de tests, mais qu'on gardait le même modèle, le score serait surement différent. De même, en choisissant un modèle dont l'apprentissage repose sur les même données, somme nous certain que l'apprentissage aurait été le même pour d'autres données ? Il est vrai que toutes les données servant à tester ne font pas parties des données d'apprentissage.\n",
    "\n",
    "Voilà tout l'intérêt des 2 prochaines méthodes qui reposent sur la même idée. Le but est d'apprendre et tester notre modèle sur toutes les données. Dans le cas du K-fold cross validation, on divise le jeu de données en K parties égales. Puis on défini notre test set comme étant l'une des $K$ parties. Les autres $K-1$ parties sont regroupées pour former le training set. On fait apprendre notre classifier. Puis on change de test set en prenant la partie suivante et on réitère $K$ fois jusqu'à ce que toutes les parties aient été un test set. \n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/K-fold_cross_validation.jpg\" style=\"width: 300px;\" />\n",
    " </div>  \n",
    " \n",
    "Il est recommandé de faire un 10-fold cross validation.\n",
    "La méthode du leave-one out cross validation correspond à la méthode du K-fold cross validation, poussé à l'extrème, pour $k=n$. En effet, à chaque itération, le test set est composé d'une seule data et le training set est composé de $n-1$ data.\n",
    "\n",
    "### 2.2 les indicateurs de performance\n",
    "<a id=\"perf\"></a>\n",
    "\n",
    "On peut utiliser différents indicateures pour mesurer la performance des algorithmes.   \n",
    "\n",
    "**Le score** nous donne tout simplement le pourcentage de réussite de classification des données de test. Il représente en fait la moyenne exacte des data prédites $xtest$ en fonction de leur label $ytest$.  \n",
    "\n",
    "**Le recal ou sensibilité** renvoie le ratio $Tp/(Tp+Fn)$ où $Tp$, true prositive et $Fn$, false negative. Il représente l'abilité du classifier de trouver les exemples positifs. C'est la probabilité de détection.  \n",
    "\n",
    "**La précision** retourne le ratio $Tp/(Tp+Fp)$ où $Fp$ false positive. Cet indicateur présente la capacité d'un classifier à ne pas labéliser positivement un label qui est négatif.  \n",
    "\n",
    "**L'erreur zero-to-one loss**, retourne le ratio d'examples mal classifiés. La meilleur performance. Il me semble que le zero-to-one loss est en fait $1-score$.  \n",
    "\n",
    "**La mean squared error**, $ MSE(y_{pred},y_{label})=\\frac{1}{n_{samples} }\\sum (y_{labeli} - y_{predi})^2 $.  \n",
    "\n",
    "**Les temps de training et de prediction**.\n",
    "\n",
    "Note : pour le recall et la précision, il faudra setter le paramètre average à macro et non micro.\n",
    "\n",
    "Voici ci-après un exemple de code avec les métriques utilisées, sur un exemple de KNN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "clf=neighbors.KNeighborsClassifier(k)\n",
    "# temps de training et prédiction mesuré\n",
    "startTrain =time.time()\n",
    "clf.fit(xtrain,ytrain)    \n",
    "endTrain = time.time()\n",
    "startpred= time.time()\n",
    "clf.predict(xtest)\n",
    "endpred = time.time()\n",
    " # Metrics (score, precision, recall, zero-to-one loss, temps de training et de predicion)\n",
    "predict = clf.predict(xtest)\n",
    "score = clf.score(xtest,ytest)\n",
    "precision =  metrics.precision_score(ytest, predict,  average='macro')\n",
    "recall = metrics.recall_score(ytest, predict, average ='macro')\n",
    "loss01 = metrics.zero_one_loss(ytest, predict)\n",
    "timetrain = endTrain - startTrain\n",
    "timePred = endpred - startpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Choix des paramètres de l'algorithme\n",
    "<a id=\"choix\"></a>\n",
    "\n",
    "#### 2.2.1 Mnist et KNN\n",
    "<a id=\"paramKNN\"></a>\n",
    "\n",
    "Nous avons réalisé plusieurs études sur KNN et les données de Mnist. Le training set était composé de 8000 données et le test set de 2000 données\n",
    "\n",
    "Tout d'abord, on a cherché à **déterminer le meilleur K** pour nos données MNIST. On a donc entrainé et testé notre KNN pour des K différents (de 2 à 15).\n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/KNN/KNN_samedata_Kchange.png\" style=\"width: 400px;\" />\n",
    " </div>  \n",
    "\n",
    "On peut constater sur le graphe ci-dessus, que le meilleur score est atteint pour K=3 (94,8%) avec une bonne précision (94,83%) ainsi que le meilleur recall (94,62%). On notera que la précision pour K=5 est légèrement meilleur(94,90%).  \n",
    "\n",
    "Les **temps de training sont faibles**(entre 0,4 et 0,7sec). Ceux de **prédiction sont plus élevés** mais globallement compris entre 22,5 et 25 sec. Les variations observées ici ne sont pas conséquentes et dépendent des autres processus s'éxécutant sur l'ordinateur. Dans l'étude ci-dessous, le temps de prédiction est plus faible. Je pense que c'est dû à mon ordinateur car j'ai mené cette étude un jour différent.  \n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/KNN/KNN_K3_15fold.png \" style=\"width: 400px;\" />\n",
    " </div>\n",
    "\n",
    "En faisant un **15-fold validation pour K=3**, on se rend compte que le modèle est plus sensible suivant les données d'entrainement. Mais cela reste correct car le score est compris entre 94% et 96%. En aucun cas le K-fold révèle un score en dessous de 94%.  \n",
    "\n",
    "Si l'on regarde l'effet de la taille du training et du test set sur le modèle pour k=3 on remarque que **plus le training set est grand, plus le score est bon**. Pour training set 10% le score est de 91%. Puis il augmente régulièrement jusque 96,5% pour un training set de 90%. De même la précision et le recall augmentent.  \n",
    "\n",
    "D'autre part, si l'on fait varier le nombre de données utilisées pour entrainer et tester le modèle, on constate que plus le training set est grand, plus le score est meilleur. Pour un training set de 4000 données, le score est de 92,8%. Pour un training set de 20000 données, le score est de 96,4%. \n",
    "\n",
    "Cependant, il faut aussi penser au temps de prédiction qui est plus important avec l'augmentation du nombre de données. C'est pour cela que l'algorithme KNN n'est pas adapté à de trop grosses quantités de données. En effet, KNN compare toutes les données de tests avec leur voisins (dans le training set), puis finalise le label. **Le temps de prédiction peut donc être très important si le training set est grand.**\n",
    "\n",
    "KNN pour K=3 semble être un bon choix de paramètre. C'est un juste milieu entre le **underfitting et le overfitting**. Dans le premier cas, si K est trop petit, on risque de mal déterminer les classe. Dans le 2eme cas, on rique de \"sur déterminer\" les classes et de trop ressérer les frontières autour d'elles. Cela peut causer des mauvaises prédictions. Il faut donc trouver le juste milieu pour que le modèle puisse bien généraliser. \n",
    "\n",
    "Mais qu'en est-il des distances présentées dans la partie 1 ?\n",
    "\n",
    "Voici les scores obtenus pour les différentes distances :\n",
    "- distance Eucliedienne, le score =  96.45 %\n",
    "- distance de Manhattan, le score =  93.65 %\n",
    "- distance de Chebyshev, le score =  68.45 %\n",
    "- distance de Hamming, le score =  74.0 %\n",
    "- distance de Minkowski p=3 , le score =  95.3 %\n",
    "- distance de Minkowski p=4, le score =  95.5 %\n",
    "- distance de Minkowski p=5, le score =  95.85 %\n",
    "\n",
    "On constate que dans le cas de mnist, la **distance euclidienne**, celle par défaut, nous donne le meilleur score. Il faut toutefois rester attentif à ce paramètre dans un autre domaine d'application.\n",
    "\n",
    "Le dernier paramètre étudié est celui des jobs. Il correspond au nombre de jobs exécutés en parallèle pour faire tourner l'algorithme KNN. -1 signifie que l'algo utilise tous les processeurs disponibles. L'utilisation de **toutes les ressources** disponible réduit le temps de prédiction : \n",
    "- temps d'entrainement 1 job :  0.49367713928222656 sec.\n",
    "- temps de prediction 1 job :  25.477837085723877 sec.\n",
    "- temps d'entrainement -1 job :  0.5066468715667725 sec.\n",
    "- temps d'prediction -1 job :  7.67464017868042 sec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Mnist et les neurones\n",
    "<a id=\"paramANN\"></a>\n",
    "\n",
    "Pour pouvoir évaluer le meilleur réseau de neurones, il faut tester différentes architectures. Premièrement, on etudie **l'effet du nombre de couches cachées**. Pour cela, on fixe le nombre de neurones sur chaque couche à 50. Et on fait varier notre réseau de 1 à 100 couches cachées.   \n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/MLP/changingHiddelayers.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    "\n",
    "\n",
    "On remarque que passer 50 hidden layers, le réseau n'arrive même plus à classifier. IL a un score stagnant autour de 11%. Mais pour un réseau ayant entre 2 et 35 couches cachées, le score varie entre 96 et 98%\n",
    "\n",
    "Ensuite on a essayer d'observer **l'effet du nombre de neurones sur chaque couche**. pour cela, on a créé 5 classifier comprenant entre 1 et 10 couches cachées et avec entre 10 et 300 neurones sur chaque couches :\n",
    "- Reseau 1 : 1 couche avec 300 neurones\n",
    "- Réseau 2 : 3 couches avec respectivement 20, 200 et 50 neurones\n",
    "- Réseau 3 : 5 couches avec 50, 100, 200, 100, 50 neurones (allure gaussienne)\n",
    "- Réseau 4 : 7 couches avec 300, 250, 200, 150, 100, 50, 10 neurones (nombres décroissants)\n",
    "- Réseau 5 : 9 couches avec 30, 60, 90,120, 150, 180, 210, 240, 270 neurones (nombres croissants)\n",
    "\n",
    "Ensuite, avec ces caractéristiques architecturales, on essaie différentes **méthodes de calcul des poids** (LBFGS, SGD et ADAM, comme vue dans la partie 1).\n",
    "Dans le graphe ci-après, les points de gauche à droites peuvent être regrouper par 3. Chaque groupe de 3 représente un réseau avec dans l'ordre des solvers LBFGS puis SGD puis Adam.\n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/MLP/weightmethod.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    "\n",
    "Pour les réseaux 1, 3 et 5, les 3 méthodes de calcul des poids donnent un résultats plus que correct (entre 96 et 98%). Le réseau 4 nous révèle ses faiblesses. On peut l'éliminer de notre étude. Le réseau 2 est un modèle acceptable uniquement si ADAM est employé.\n",
    "De façon générale, on remarque que la méthode LBFGS est plus lente à apprendre, avec un temps d'entrainement pouvant dépasser les 10 min pour 49000 données. C'est la méthode ADAM qui a le temps d'apprentissage le plus faible, pour les 5 réseaux testés.\n",
    "\n",
    "On peut aussi comparer ces 5 réseaux en changeant leur **fonction d'activation**. On étudie ici les fonctions d'activation suivantes : identity, logistic, tanh et relu (cf partie 1 pour une présentation théorique). De gauche à droite, chaque groupe de 4 points représentes un réseau. \n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/MLP/activationfunction.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    "\n",
    "Ici c'est le réseau 5 qui n'a pas réussi à classifier, avec la fonction d'activation logistique. On remarque globalement que c'est la focntion d'activation Relu, celle par défaut qui donnele meilleur score, la meilleur précision et le meilleur recall. Pour les temps d'apprentissage, c'est plus mitigé et cela dépend des réseaux. Avec la fonction d'activation logistique, le temps d'apprentissage est souvent plus élevé. Et avec Relu il est souvent plus faible.\n",
    "\n",
    "On peut aussi faire varier le alpha qui sert à évitter l'overfitting. Un alpha faible encourragera des poids et un biais élevés, résultant en des frontières de classification plus lisses. Tandis qu'un alpha élevé encouragera des poids plus petits, résultat en des frontières potentiellement plus compliquées.\n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/MLP/changingAlpha.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    "\n",
    "\n",
    "Sur nos 5 réseaux, on constate que un alpha trop élevé (1000) ne permet plus de classifier nos données. Le score chute à 11%. En regardant les temps d'apprentissage, un faible alpha (0,001 ou 0,1) est préférable.\n",
    "\n",
    "\n",
    "\n",
    "Avec l'étude sur ces 5 réseaux, on remaque que le réseau 4 qui a un nombre croissant de neurones sur chaque couche (forme de pyramide) n'est pas idéal. Au contraire avoir un pyramide inversée, comme avec le réseau 5, ou une forme losange comme le **réseau 3**, offre un bon score de classification. Pour les méthodes de poids, il faut privilégier **ADAM** qui offre le meilleur score pour le temps d'apprentissage le plus faible. De même, la fonction d'activation par défaut, **relu**, semble être la fonction la plus adéquate. Enfin, il faut privilégier un **alpha entre 0,001 et 0,1** pour ne pas avoir de hunderfitting et ne plus pouvoir classifier nos données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Mnist et SVM\n",
    "<a id=\"paramSVM\"></a>\n",
    "\n",
    "Pour etudier le modèle SVC de scikitlearn, sur mnist, on commence par étudier les différents **kernels**.\n",
    "\n",
    "<div class=\"raw\">\n",
    "        <img src=\"images_rapport/SVM/kernel.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    " On constate ici que les kernel linéaire et poly ont un score satisfaisant (98,6% et 99,5%), comparé aux kernel rbf et sigmoid. De même, leur temps d'apprentisage et de prédiction sont très faibles (environ 11 et 5 sec).\n",
    " \n",
    " Maintenant, on fait varier le paramère **C**, avec un kernel poly :\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/SVM/changeC.png\" style=\"width: 400px;\" />\n",
    " </div>\n",
    " \n",
    "Pour C, rien ne change, le score reste à 99,56%, seul le temps d'entrainement et de prédiction varient. C'est à cause des autres processus s'exécutant sur mon ordinateur.\n",
    "\n",
    "En faisant varier **gamma**, on a les même constats. Le graphe n'est donc pas rapporté ici, pour ne pas surcharger le rapport.\n",
    "\n",
    "Sikit-learn nous offre un outils : GridSearchCV, qui va trouver la meilleur combinaison suivant les paramètres qu'on lui donne. (Le seul inconvénient de cet outil est qu'il est long à exécuter). Voici donc comment l'exécuter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTIE AVEC GridSearchCV\n",
    "\n",
    "paramGrid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.0001, 0.001, 0.01, 0.1],\n",
    "              'kernel': ('linear', 'poly')\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(SVC(), paramGrid, cv=5)\n",
    "grid = grid.fit(X=xtrain, y=ytrain)\n",
    "print (grid.best_params_)\n",
    "\n",
    "#The best param are : {'C': 0.001, 'gamma': 0.0001, 'kernel': 'linear'} for 10000 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les meilleurs paramètres choisi pour 10000 données, on le fait tourner en augmentant les données, jusqu'à 50000. On se rend compte que le modèle ne généralise pas bien. Il faudrait refaire une études pour changer les paramètres pour des données plus importantes. Cependant, cela demande du temps.\n",
    "\n",
    " <div class=\"raw\">\n",
    "        <img src=\"images_rapport/SVM/augmentData.png\" style=\"width: 400px;\" />\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse comparative\n",
    "<a id=\"comp\"></a>\n",
    "Pour pouvoir comparer les 3 méthodes rappelons les performances obtenues :\n",
    "- pour KNN : pour 20000 données, 96,5% pour un temps de training négligeable et un temps de prédiction de 30sec\n",
    "- pour ANN : pour 70000 données 98,5%, un temps d'entrainement de 5min et un temps de prédiction négligeable\n",
    "- pour SVM : pour 20000 données, un score de 98,78% pour un temps de training de 10 sec et un temps de prédiction de 5 sec.\n",
    "\n",
    "A première vu, SVM est donc le meilleur modèle en fonction du score et des temps d'entrainement et de training. Mais on a aussi vu que SVM ne généralisait pas bien car si l'on garde le même modèle, on tombe à 96% pour des temps d'entrainement et de prédiction de 4min.\n",
    "\n",
    "On remarque aussi qu'entre KNN et MLP, le temps d'apprentissage et de prediction ont été inversé. En effet, KNN apprend très rapidement, mais il doit tester tous les voisins avant de pouvoir labéliser les données de test donc son temps de prédiction  est long. Inversement, le Réseau de neuronne doit stabiliser ses poids à l'entrainement puis la prédiction est instantannée.\n",
    "\n",
    "En ce sens, ANN est à préconnisé si on l'adapte pour des données reçues en streaming, ou si l'on doit prédire un grand nombre de données. Au contraire, on ne recommandera pas KNN qui est trop coûteux en temps de prédiction.\n",
    "\n",
    "Globalement, les 3 modèles permettent bien de classifier nos données mnist.\n",
    "\n",
    "Personnellement, je suis plus à l'aise avec les modèles de KNN et ANN. J'ai du mal à voir les effets de C et de gamma dans le modèle de SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meilleur KNN\n",
    "# K=3\n",
    "# Toutes les ressources (njobs=-1)\n",
    "# distance euclidienne par défaut\n",
    "\n",
    "# imports mnist et KNN\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import neighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "mnist=datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "\n",
    "#ECHANTILLONS DE 10000\n",
    "indices = np.random.randint(70000, size=20000)\n",
    "data = mnist.data[indices]\n",
    "target = mnist.target[indices]\n",
    "#pour séparer le dataset en 2 échantillons de trainint et de test\n",
    "xtrain, xtest, ytrain, ytest =train_test_split(data, target, train_size=0.8)\n",
    "\n",
    "\n",
    "#Classifier\n",
    "clf=neighbors.KNeighborsClassifier(3,n_jobs=-1)\n",
    "# temps de training et prédiction mesuré\n",
    "startTrain =time.time()\n",
    "clf.fit(xtrain,ytrain)    \n",
    "endTrain = time.time()\n",
    "startpred= time.time()\n",
    "clf.predict(xtest)\n",
    "endpred = time.time()\n",
    " # Metrics (score, precision, recall, zero-to-one loss, temps de training et de predicion)\n",
    "predict = clf.predict(xtest)\n",
    "score = clf.score(xtest,ytest)\n",
    "precision =  metrics.precision_score(ytest, predict,  average='macro')\n",
    "recall = metrics.recall_score(ytest, predict, average ='macro')\n",
    "loss01 = metrics.zero_one_loss(ytest, predict)\n",
    "timetrain = endTrain - startTrain\n",
    "timePred = endpred - startpred\n",
    "\n",
    "print(\"Ce modèle de KNN, K=3, à un score de \", score*100, \"%.\")\n",
    "print(\"4eme image : prédiction \",predict[3], \"reel : \", ytest[3])\n",
    "print (\"ce KNN à une précision de\", precision*100, \"%.\")\n",
    "print (\"ce KNN à un recall de\",recall*100, \"%.\")\n",
    "print (\"ce KNN à un zero-one_loss de\",recall*100, \"%.\")\n",
    "print(\"    temps apprentissage : \", timetrain, \"sec , temps prediction = \", timePred, \"sec.\" )\n",
    "\n",
    "#Ce modèle de KNN, K=3, à un score de  95.75 %.\n",
    "#4eme image : prédiction  2.0 reel :  2.0\n",
    "#ce KNN à une précision de 95.8171550275235 %.\n",
    "#ce KNN à un recall de 95.69581521050152 %.\n",
    "#ce KNN à un zero-one_loss de 95.69581521050152 %.\n",
    "#    temps apprentissage :  1.5378844738006592 sec , temps prediction =  29.139420747756958 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEILLEUR MLP \n",
    "# réseau 3 (50, 100, 200, 100,50)\n",
    "# Relu et Adam (par défaut)\n",
    "# petit alpha 0,1\n",
    "\n",
    "#import necessaires\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "mnist=datasets.fetch_mldata('MNIST original')\n",
    "#randomise Data et target\n",
    "indices = np.random.randint(70000, size=70000)\n",
    "data = mnist.data[indices]\n",
    "target = mnist.target[indices]\n",
    "# on veut un training set de 49000\n",
    "xtrain, xtest, ytrain, ytest =train_test_split(data, target, train_size=49000)\n",
    "\n",
    "clf3 = MLPClassifier(hidden_layer_sizes=(50, 100, 200, 100,50),alpha= 0.1) # defaut ADAM et Relu\n",
    "startTrain =time.time()\n",
    "clf3.fit(xtrain, ytrain)\n",
    "endTrain = time.time()\n",
    "# Predict\n",
    "startpred= time.time()\n",
    "predict = clf3.predict(xtest)\n",
    "endpred = time.time()\n",
    "score = clf3.score(xtest,ytest)\n",
    "\n",
    "recall = metrics.recall_score(ytest, predict, average ='macro')\n",
    "precision = metrics.precision_score(ytest, predict,  average='macro')\n",
    "loss01 = metrics.zero_one_loss(ytest, predict)\n",
    "timetrain = endTrain - startTrain\n",
    "timePred = endpred - startpred\n",
    "\n",
    "\n",
    "print(\"Ce modèle de MLP, de 5 couches de 50, 100, 200, 100, 50, a un score de \", score*100, \"%.\")\n",
    "print(\"4eme image : prédiction \",predict[3], \"reel : \", ytest[3])\n",
    "print (\"ce modèle de MLP à une précision de\", precision*100, \"%.\")\n",
    "print (\"ce modèle de MLP à un recall de\",recall*100, \"%.\")\n",
    "print (\"ce modèle de MLP à un zero-one_loss de\",recall*100, \"%.\")\n",
    "print(\"    temps apprentissage : \", timetrain, \"sec , temps prediction = \", timePred, \"sec.\" )\n",
    "\n",
    "#Ce modèle de MLP, d' 5 couches de 50, 100, 200, 100, 50, a un score de  98.5047619047619 %.\n",
    "#4eme image : prédiction  5.0 reel :  5.0\n",
    "#ce modèle de MLP à une précision de 98.50503688809177 %.\n",
    "#ce modèle de MLP à un recall de 98.49104431028593 %.\n",
    "#ce modèle de MLP à un zero-one_loss de 98.49104431028593 %.\n",
    "#    temps apprentissage :  322.9891335964203 sec , temps prediction =  0.6702065467834473 sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "<a id=\"conclusion\"></a>\n",
    "Ce rapport propose des paramétrage de 3 méthodes d'apprentissage supervisé pour classifier les données mnist. \n",
    "Les 3 méthodes portent leur fruits et présentes toutes des avantages et inconvénient.\n",
    "\n",
    "Il serait utile de poursuivre l'étude en l'étendant à d'autres méthodes supervisées comme le décision tree, les réseaux baésian, les CNN et RNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Éditer les Méta-Données",
  "ipub": {
   "author": "Anaïs RABARY",
   "date": "2018-12-10",
   "subtitle": "Etude comparative des méthodes KNN, ANN et MLP",
   "title": "Rapport TPs Apprentissage Supervisé",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_metadata": {
   "author": "Anaïs RABARY",
   "date": "2018-12-10",
   "output": {
    "pdf_document": {
     "toc": true
    }
   },
   "subtitle": "Etude comparative des méthodes KNN, ANN et MLP",
   "title": "Rapport TPs Apprentissage Supervisé",
   "toc": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
